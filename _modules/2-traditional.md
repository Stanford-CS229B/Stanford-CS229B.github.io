---
title: Week 2,3,4 - Traditional time series methods
---

Oct 2
: **Lecture 2**{: .label .label-purple }
: **CQ1 out**{: .label .label-green }
: * Stationarity, autocorrelation
  * Foundational stationary time series models -- Part 1 (AR and MA processes)
  * Reading:
      1. S&S 1.3-1.5; S&S 3.1 (up to ARMA)

Oct 4
: **Lecture 3**{: .label .label-purple }
: **CQ1 due**{: .label .label-green }
: * Foundational stationary time series models -- Part 2 (ARMA processes)
  * Forecasting
  * Reading:
      1. S&S 3.1 (remainder); S&S 3.3-3.4

Oct 9
: **Lecture 4**{: .label .label-purple }
: * Estimating ARMA models
  * Foundational non-stationary time series models (ARIMA/SARIMA)
  * Multivariate processes
  * Reading:
      1. S&S 3.5-3.7; 3.9; 5.6 (high level)
      2. (optional) Lutkepohl 2.1 (VAR); 11.1-11.3 (VARMA)

Oct 11
: **Lecture 5**{: .label .label-purple }
: **HW1 due**{: .label .label-orange }
: **HW2 out**{: .label .label-orange }
: **CQ2 out**{: .label .label-green }
: * State space models (SSMs)
  * Kalman filtering/smoothing
  * Dynamic latent factor models
  * Reading:
      1. S&S 6.1-6.2
      2. Murphy 29.6-29.8.3; 8.1-8.2
      3. (optional) Lutkepohl 18.1-18.4 (SSMs, filtering/smoothing)
      4. (optional) Bishop 13.3 (reading 13.2 first will help)

Oct 16
: **Lecture 6**{: .label .label-purple }
: * Hidden Markov models (HMMs)
  * Learning and inference in HMMs
  * Reading:
      1. S&S 6.3, 6.9
      2. Murphy 29.1-29.4.2; 9.2
      3. (optional) Bishop 13.2-13.3 (cont'd); 9.2-9.3 (EM background)

Oct 18
: **Lecture 7**{: .label .label-purple }
: **CQ2 due**{: .label .label-green }
: * Learning SSMs cont'd (EM algorithm)
  * Switching SSMs
  * Reading:
      1. S&S 6.10
      2. Murphy 29.9
