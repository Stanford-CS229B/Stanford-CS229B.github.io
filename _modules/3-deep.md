---
title: Week 5 - Traditional Time Series Methods
---

Oct 23
: **Lecture 8**{: .label .label-purple }
  : **project proposal due**{: .label .label-red }
: * Refresher on feedforward neural networks, backpropagation
  * Autoregressive and recurrent neural networks (RNNs)
  * Reading:
        1. GBC Ch.6; Ch.10.1-10.5 (excluding 10.2.2)
        2. (optional) Murphy 16.1-16.3

Oct 25
: **Lecture 9**{: .label .label-purple }
  : **CQ3 out**{: .label .label-green }
: * Backpropagation through time
  * Gated RNNs (LSTMs, GRUs)
  * Reading:
        1. GBC 10.2.2; 10.7-10.12
        2. (optional) Murphy 16.1-16.3
        3. (optional) [Amazon research WWW 2020 tutorial on RNNs and CNNs for time series](https://lovvge.github.io/Forecasting-Tutorial-WWW-2020/)
        4. (optional) [Deep AR](https://www.sciencedirect.com/science/article/pii/S0169207019301888)
        5. (optional) [N-BEATS](https://arxiv.org/pdf/1905.10437.pdf)

Oct 30
: **Lecture 9**{: .label .label-purple }
: * Refresher on convolutional neural networks (CNNs)
  * CNNs for sequential data; Transformers
  * Reading:
        1. GBC Ch. 9
        2. (optional) [Attention Is All You Need](https://arxiv.org/abs/1706.03762)
        3. (optional) [The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/)
        4. (optional) [Transformer (Google AI blog post)](https://blog.research.google/2017/08/transformer-novel-neural-network.html)
        5. (optional) [Transformers for time series forecasting](https://arxiv.org/pdf/1907.00235.pdf)
        6. (optional) [More on attention for those interested](https://towardsdatascience.com/attn-illustrated-attention-5ec4ad276ee3)
